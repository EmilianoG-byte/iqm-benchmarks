{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the `jax.grad` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VJP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[-0.42147675-0.17252079j -0.33669758-0.02341664j  0.77379787-0.578864j  ]\n"
     ]
    }
   ],
   "source": [
    "# First, let us start testing the gradient of the non-holomorphic function from figure 3 in KrÃ¤mer paper.\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey (1)\n",
    "key1 , key2 , key3 = jax.random.split(key , num=3) \n",
    "\n",
    "A = jax.random.normal(key1 , (3, 3), dtype=complex)\n",
    "x = jax.random.normal(key2 , (3,), dtype=complex)\n",
    "\n",
    "# Had to take the real part so that the gradient works correctly.\n",
    "def function(v):\n",
    "    return jnp.real(jnp.dot(v.conj(), A@v))\n",
    "\n",
    "# Complex gradient should be of shape (3,). Remember that we should take the conjugate due to the convention in JAX.\n",
    "grad_x = jax.grad(function)(x)\n",
    "\n",
    "# this is the \\bar{f} in section 6. Should be: df = du + i dv. Satisying equation 24\n",
    "# Is a vector of the same shape as the output of f (scalar in this case)\n",
    "df = 1.0\n",
    "_, vjp_ad = jax.vjp(function, x)\n",
    "(dx_ad, ) = vjp_ad(1.0)\n",
    "\n",
    "# Applying the VJP to df is equal to the latent gradient as expected!\n",
    "print(jnp.allclose(grad_x, dx_ad))\n",
    "\n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[-0.42147675-0.17252079j -0.33669758-0.02341664j  0.77379787-0.578864j  ]\n"
     ]
    }
   ],
   "source": [
    "# let me try to calculate the gradient again, but now without taking the real part!\n",
    "def complex_function(v):\n",
    "    return jnp.dot(v.conj(), A@v)\n",
    "# Need to make it complex to align with the type of the output of f!\n",
    "df_complex = jnp.complex64(1.0)\n",
    "\n",
    "_, vjp_ad_complex = jax.vjp(complex_function, x)\n",
    "(dx_ad_complex, ) = vjp_ad_complex(df_complex)\n",
    "print(jnp.allclose(dx_ad_complex, grad_x))\n",
    "# Just as we expected, they are the same ðŸ¥³\n",
    "print(dx_ad_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42147675+0.17252079j -0.33669758+0.02341664j  0.77379787+0.578864j  ]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Important question, is this the correct result? Let's compare with the analytical result.\n",
    "# Apparently the derivative is this expression:\n",
    "\n",
    "def vjp_analytical(df):\n",
    "    return (A.conj().T @ x * df) + (A @ x * df.conj())\n",
    "\n",
    "grad_analytical = vjp_analytical(jnp.complex64(df))\n",
    "print(grad_analytical)\n",
    "\n",
    "# We see from this that the gradient we get from jax is the conjugate of what we expect analytically!\n",
    "print(jnp.allclose(grad_analytical, grad_x))\n",
    "# It is important then that we take the conjugate!\n",
    "print(jnp.allclose(grad_analytical.conj(), grad_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `JVP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.7417455-3.6587248j)\n",
      "1.7417455\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Just testing the JVP works for complex numbers\n",
    "dx = jax.random.normal(key3 , (3,) , dtype=complex) #2*x\n",
    "# First for the complex function\n",
    "jvp_test_jax = jax.jvp(complex_function,(x,), (dx,))[1]\n",
    "print(jvp_test_jax)\n",
    "# Then for teh real function\n",
    "jvp_test_jax_real = jax.jvp(function,(x,), (dx,))[1]\n",
    "print(jvp_test_jax_real)\n",
    "# Interestingly, they are not the same!\n",
    "print(jnp.allclose(jvp_test_jax_real, jvp_test_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.30086893, dtype=float32),\n",
       " Array(0.30086893-0.32736388j, dtype=complex64))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well there is indeed a difference between the two functions!\n",
    "function(x), complex_function(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "Indeed, here it does not make sense to take the real part, because we are not dealing with a Hermitian matrix (observable). Then the `complex_function` is the more correct thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(1.7417456-0.3855983j)\n",
      "False\n",
      "(1.7417456-0.3855983j)\n"
     ]
    }
   ],
   "source": [
    "# We see this is (NOT??) the same result as the (real) inner product between the gradient and the same input vector (2x here)\n",
    "jvp_test_grad = jnp.dot(grad_x, dx)\n",
    "print(jnp.allclose(jvp_test_grad, jvp_test_jax))\n",
    "print(jvp_test_grad)\n",
    "\n",
    "# conjugate version?\n",
    "grad_x_conj = dx_ad_complex\n",
    "jvp_test_grad_conj = jnp.dot(grad_x_conj, dx)\n",
    "print(jnp.allclose(jvp_test_grad_conj, jvp_test_jax))\n",
    "print(jvp_test_grad_conj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1.7417455-3.6587248j, dtype=complex64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But are they the correct result? Let's find out:\n",
    "def jvp_analytical(z, z_tilde):\n",
    "    # the dot function takes care of the tranpose\n",
    "    dw1 = jnp.dot(z_tilde.conj () , A @ z )\n",
    "    dw2 = jnp.dot(z.conj(), A @ z_tilde)\n",
    "    return dw1 + dw2\n",
    "\n",
    "# Apparently they are! Why is the grad * dx not then?\n",
    "jvp_test_analytical = jvp_analytical(x, dx)\n",
    "print(jnp.allclose(jvp_test_analytical, jvp_test_jax))\n",
    "jvp_test_analytical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcvv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
