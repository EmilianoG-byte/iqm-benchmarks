{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the `jax.grad` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VJP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[-0.42147675-0.17252079j -0.33669758-0.02341664j  0.77379787-0.578864j  ]\n"
     ]
    }
   ],
   "source": [
    "# First, let us start testing the gradient of the non-holomorphic function from figure 3 in KrÃ¤mer paper.\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey (1)\n",
    "key1 , key2 , key3 = jax.random.split(key , num=3) \n",
    "\n",
    "A = jax.random.normal(key1 , (3, 3), dtype=complex)\n",
    "x = jax.random.normal(key2 , (3,), dtype=complex)\n",
    "\n",
    "# Had to take the real part so that the gradient works correctly.\n",
    "def function(v):\n",
    "    return jnp.real(jnp.dot(v.conj(), A@v))\n",
    "\n",
    "# Complex gradient should be of shape (3,). Remember that we should take the conjugate due to the convention in JAX.\n",
    "grad_x = jax.grad(function)(x)\n",
    "\n",
    "# this is the \\bar{f} in section 6. Should be: df = du + i dv. Satisying equation 24\n",
    "# Is a vector of the same shape as the output of f (scalar in this case)\n",
    "df = 1.0\n",
    "_, vjp_ad = jax.vjp(function, x)\n",
    "(dx_ad, ) = vjp_ad(1.0)\n",
    "\n",
    "# Applying the VJP to df is equal to the latent gradient as expected!\n",
    "print(jnp.allclose(grad_x, dx_ad))\n",
    "\n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[-0.42147675-0.17252079j -0.33669758-0.02341664j  0.77379787-0.578864j  ]\n"
     ]
    }
   ],
   "source": [
    "# let me try to calculate the gradient again, but now without taking the real part!\n",
    "def complex_function(v):\n",
    "    return jnp.dot(v.conj(), A@v)\n",
    "# Need to make it complex to align with the type of the output of f!\n",
    "df_complex = jnp.complex64(1.0)\n",
    "\n",
    "_, vjp_ad_complex = jax.vjp(complex_function, x)\n",
    "(dx_ad_complex, ) = vjp_ad_complex(df_complex)\n",
    "print(jnp.allclose(dx_ad_complex, grad_x))\n",
    "# Just as we expected, they are the same ðŸ¥³\n",
    "print(dx_ad_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad requires real-valued outputs (output dtype that is a sub-dtype of np.floating), but got complex64. For holomorphic differentiation, pass holomorphic=True. For differentiation of non-holomorphic functions involving complex outputs, use jax.vjp directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grad_x_complex \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/qcvv-env/lib/python3.11/site-packages/jax/_src/api.py:521\u001b[0m, in \u001b[0;36m_check_output_dtype_revderiv\u001b[0;34m(name, holomorphic, x)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with holomorphic=True requires outputs with complex dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mcomplexfloating):\n\u001b[0;32m--> 521\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires real-valued outputs (output dtype that is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma sub-dtype of np.floating), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor holomorphic differentiation, pass holomorphic=True. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor differentiation of non-holomorphic functions involving complex \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs, use jax.vjp directly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m    527\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires real-valued outputs (output dtype that is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma sub-dtype of np.floating), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor differentiation of functions with integer outputs, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.vjp directly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: grad requires real-valued outputs (output dtype that is a sub-dtype of np.floating), but got complex64. For holomorphic differentiation, pass holomorphic=True. For differentiation of non-holomorphic functions involving complex outputs, use jax.vjp directly."
     ]
    }
   ],
   "source": [
    "# cannot obtain the gradient of a complex valued function by definition from jax\n",
    "grad_x_complex = jax.grad(complex_function)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42147675+0.17252079j -0.33669758+0.02341664j  0.77379787+0.578864j  ]\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Important question, is this the correct result? Let's compare with the analytical result.\n",
    "# Apparently the derivative is this expression:\n",
    "\n",
    "def vjp_analytical(df):\n",
    "    return (A.conj().T @ x * df) + (A @ x * df.conj())\n",
    "\n",
    "grad_analytical = vjp_analytical(jnp.complex64(df))\n",
    "print(grad_analytical)\n",
    "\n",
    "# We see from this that the gradient we get from jax is the conjugate of what we expect analytically!\n",
    "print(jnp.allclose(grad_analytical, grad_x))\n",
    "# It is important then that we take the conjugate!\n",
    "print(jnp.allclose(grad_analytical.conj(), grad_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `JVP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.7417455-3.6587248j)\n",
      "1.7417455\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Just testing the JVP works for complex numbers\n",
    "dx = jax.random.normal(key3 , (3,) , dtype=complex) #2*x\n",
    "# First for the complex function\n",
    "jvp_test_jax = jax.jvp(complex_function,(x,), (dx,))[1]\n",
    "print(jvp_test_jax)\n",
    "# Then for teh real function\n",
    "jvp_test_jax_real = jax.jvp(function,(x,), (dx,))[1]\n",
    "print(jvp_test_jax_real)\n",
    "# Interestingly, they are not the same!\n",
    "print(jnp.allclose(jvp_test_jax_real, jvp_test_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.30086893, dtype=float32),\n",
       " Array(0.30086893-0.32736388j, dtype=complex64))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well there is indeed a difference between the two functions!\n",
    "function(x), complex_function(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "Indeed, here it does not make sense to take the real part, because we are not dealing with a Hermitian matrix (observable). Then the `complex_function` is the more correct thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(1.7417456-0.3855983j)\n",
      "False\n",
      "(1.7417456-0.3855983j)\n"
     ]
    }
   ],
   "source": [
    "# We see this is (NOT??) the same result as the (real) inner product between the gradient and the same input vector (2x here)\n",
    "jvp_test_grad = jnp.dot(grad_x, dx)\n",
    "print(jnp.allclose(jvp_test_grad, jvp_test_jax))\n",
    "print(jvp_test_grad)\n",
    "\n",
    "# conjugate version?\n",
    "grad_x_conj = dx_ad_complex\n",
    "jvp_test_grad_conj = jnp.dot(grad_x_conj, dx)\n",
    "print(jnp.allclose(jvp_test_grad_conj, jvp_test_jax))\n",
    "print(jvp_test_grad_conj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(1.7417455-3.6587248j, dtype=complex64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But are they the correct result? Let's find out:\n",
    "def jvp_analytical(z, z_tilde):\n",
    "    # the dot function takes care of the tranpose\n",
    "    dw1 = jnp.dot(z_tilde.conj () , A @ z )\n",
    "    dw2 = jnp.dot(z.conj(), A @ z_tilde)\n",
    "    return dw1 + dw2\n",
    "\n",
    "# Apparently they are! Why is the grad * dx not then?\n",
    "jvp_test_analytical = jvp_analytical(x, dx)\n",
    "print(jnp.allclose(jvp_test_analytical, jvp_test_jax))\n",
    "jvp_test_analytical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcvv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
